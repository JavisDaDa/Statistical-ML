{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best sigma, C, lr are (10.000000, 3000.000000, 0.003000) with an accuracy on the validation set of 0.972500.\n",
      "iteration 0 / 30000: loss 3000.000000\n",
      "iteration 100 / 30000: loss 1630848.964543\n",
      "iteration 200 / 30000: loss 306210.601406\n",
      "iteration 300 / 30000: loss 165081.018428\n",
      "iteration 400 / 30000: loss 116685.269818\n",
      "iteration 500 / 30000: loss 89709.983528\n",
      "iteration 600 / 30000: loss 1613583.118307\n",
      "iteration 700 / 30000: loss 185175.453933\n",
      "iteration 800 / 30000: loss 103825.061112\n",
      "iteration 900 / 30000: loss 82158.074935\n",
      "iteration 1000 / 30000: loss 67043.585306\n",
      "iteration 1100 / 30000: loss 52923.482294\n",
      "iteration 1200 / 30000: loss 43203.201252\n",
      "iteration 1300 / 30000: loss 126767.862183\n",
      "iteration 1400 / 30000: loss 71267.704709\n",
      "iteration 1500 / 30000: loss 55699.873664\n",
      "iteration 1600 / 30000: loss 44664.472627\n",
      "iteration 1700 / 30000: loss 37997.205253\n",
      "iteration 1800 / 30000: loss 33699.782016\n",
      "iteration 1900 / 30000: loss 86136.996571\n",
      "iteration 2000 / 30000: loss 50797.534175\n",
      "iteration 2100 / 30000: loss 39096.880056\n",
      "iteration 2200 / 30000: loss 33247.653492\n",
      "iteration 2300 / 30000: loss 35413.888173\n",
      "iteration 2400 / 30000: loss 1227213.118981\n",
      "iteration 2500 / 30000: loss 43259.554222\n",
      "iteration 2600 / 30000: loss 33325.512313\n",
      "iteration 2700 / 30000: loss 28860.130325\n",
      "iteration 2800 / 30000: loss 26344.657009\n",
      "iteration 2900 / 30000: loss 24590.440875\n",
      "iteration 3000 / 30000: loss 24910.678323\n",
      "iteration 3100 / 30000: loss 24267.397672\n",
      "iteration 3200 / 30000: loss 20922.316290\n",
      "iteration 3300 / 30000: loss 20141.108269\n",
      "iteration 3400 / 30000: loss 19444.430104\n",
      "iteration 3500 / 30000: loss 18710.438753\n",
      "iteration 3600 / 30000: loss 18279.869623\n",
      "iteration 3700 / 30000: loss 17514.333677\n",
      "iteration 3800 / 30000: loss 16961.839633\n",
      "iteration 3900 / 30000: loss 16450.522605\n",
      "iteration 4000 / 30000: loss 15927.410277\n",
      "iteration 4100 / 30000: loss 15426.126497\n",
      "iteration 4200 / 30000: loss 14847.298593\n",
      "iteration 4300 / 30000: loss 14423.034351\n",
      "iteration 4400 / 30000: loss 13957.418265\n",
      "iteration 4500 / 30000: loss 13615.847932\n",
      "iteration 4600 / 30000: loss 13013.207698\n",
      "iteration 4700 / 30000: loss 12647.672440\n",
      "iteration 4800 / 30000: loss 12288.348498\n",
      "iteration 4900 / 30000: loss 11911.430076\n",
      "iteration 5000 / 30000: loss 11600.072333\n",
      "iteration 5100 / 30000: loss 11321.342680\n",
      "iteration 5200 / 30000: loss 10955.642246\n",
      "iteration 5300 / 30000: loss 10701.195024\n",
      "iteration 5400 / 30000: loss 10299.841690\n",
      "iteration 5500 / 30000: loss 10107.853971\n",
      "iteration 5600 / 30000: loss 9776.140716\n",
      "iteration 5700 / 30000: loss 9431.671584\n",
      "iteration 5800 / 30000: loss 9087.944057\n",
      "iteration 5900 / 30000: loss 12567.473671\n",
      "iteration 6000 / 30000: loss 8578.257888\n",
      "iteration 6100 / 30000: loss 8279.185503\n",
      "iteration 6200 / 30000: loss 8078.805197\n",
      "iteration 6300 / 30000: loss 8018.574052\n",
      "iteration 6400 / 30000: loss 7750.364320\n",
      "iteration 6500 / 30000: loss 7445.022611\n",
      "iteration 6600 / 30000: loss 7298.608109\n",
      "iteration 6700 / 30000: loss 6923.100026\n",
      "iteration 6800 / 30000: loss 6730.285863\n",
      "iteration 6900 / 30000: loss 6508.420010\n",
      "iteration 7000 / 30000: loss 6309.910419\n",
      "iteration 7100 / 30000: loss 6112.246468\n",
      "iteration 7200 / 30000: loss 5926.864101\n",
      "iteration 7300 / 30000: loss 410560.586082\n",
      "iteration 7400 / 30000: loss 36104.111740\n",
      "iteration 7500 / 30000: loss 22774.208587\n",
      "iteration 7600 / 30000: loss 15451.505986\n",
      "iteration 7700 / 30000: loss 12114.930372\n",
      "iteration 7800 / 30000: loss 10536.515168\n",
      "iteration 7900 / 30000: loss 9490.657114\n",
      "iteration 8000 / 30000: loss 8781.334536\n",
      "iteration 8100 / 30000: loss 8417.763095\n",
      "iteration 8200 / 30000: loss 7881.225345\n",
      "iteration 8300 / 30000: loss 7521.187196\n",
      "iteration 8400 / 30000: loss 7236.619337\n",
      "iteration 8500 / 30000: loss 10870.996759\n",
      "iteration 8600 / 30000: loss 6801.351952\n",
      "iteration 8700 / 30000: loss 6560.557620\n",
      "iteration 8800 / 30000: loss 6344.342623\n",
      "iteration 8900 / 30000: loss 6139.527768\n",
      "iteration 9000 / 30000: loss 5948.627479\n",
      "iteration 9100 / 30000: loss 5799.290500\n",
      "iteration 9200 / 30000: loss 5539.874392\n",
      "iteration 9300 / 30000: loss 5417.324319\n",
      "iteration 9400 / 30000: loss 5152.390433\n",
      "iteration 9500 / 30000: loss 5172.095068\n",
      "iteration 9600 / 30000: loss 4880.159453\n",
      "iteration 9700 / 30000: loss 9383.206153\n",
      "iteration 9800 / 30000: loss 4590.813417\n",
      "iteration 9900 / 30000: loss 5102.604448\n",
      "iteration 10000 / 30000: loss 4370.069919\n",
      "iteration 10100 / 30000: loss 4623.667726\n",
      "iteration 10200 / 30000: loss 4464.999391\n",
      "iteration 10300 / 30000: loss 7717.613016\n",
      "iteration 10400 / 30000: loss 51229.571473\n",
      "iteration 10500 / 30000: loss 23407.795510\n",
      "iteration 10600 / 30000: loss 13842.044128\n",
      "iteration 10700 / 30000: loss 9953.362498\n",
      "iteration 10800 / 30000: loss 8100.899013\n",
      "iteration 10900 / 30000: loss 6956.560261\n",
      "iteration 11000 / 30000: loss 6080.718185\n",
      "iteration 11100 / 30000: loss 5982.857181\n",
      "iteration 11200 / 30000: loss 5393.456691\n",
      "iteration 11300 / 30000: loss 5149.254882\n",
      "iteration 11400 / 30000: loss 4955.149809\n",
      "iteration 11500 / 30000: loss 4734.708571\n",
      "iteration 11600 / 30000: loss 4920.123790\n",
      "iteration 11700 / 30000: loss 4569.689879\n",
      "iteration 11800 / 30000: loss 4483.561351\n",
      "iteration 11900 / 30000: loss 4220.522195\n",
      "iteration 12000 / 30000: loss 4251.863841\n",
      "iteration 12100 / 30000: loss 4088.494163\n",
      "iteration 12200 / 30000: loss 4006.980785\n",
      "iteration 12300 / 30000: loss 3763.536671\n",
      "iteration 12400 / 30000: loss 3844.498942\n",
      "iteration 12500 / 30000: loss 3659.333407\n",
      "iteration 12600 / 30000: loss 3450.935322\n",
      "iteration 12700 / 30000: loss 3342.964336\n",
      "iteration 12800 / 30000: loss 3397.572190\n",
      "iteration 12900 / 30000: loss 3238.612036\n",
      "iteration 13000 / 30000: loss 3052.627253\n",
      "iteration 13100 / 30000: loss 3099.924645\n",
      "iteration 13200 / 30000: loss 3089.855835\n",
      "iteration 13300 / 30000: loss 2958.050163\n",
      "iteration 13400 / 30000: loss 9309.027638\n",
      "iteration 13500 / 30000: loss 2747.265067\n",
      "iteration 13600 / 30000: loss 2676.593711\n",
      "iteration 13700 / 30000: loss 5942.175504\n",
      "iteration 13800 / 30000: loss 2514.036109\n",
      "iteration 13900 / 30000: loss 2473.626035\n",
      "iteration 14000 / 30000: loss 2438.385424\n",
      "iteration 14100 / 30000: loss 2488.357785\n",
      "iteration 14200 / 30000: loss 2478.449249\n",
      "iteration 14300 / 30000: loss 2442.285762\n",
      "iteration 14400 / 30000: loss 2273.978654\n",
      "iteration 14500 / 30000: loss 2311.371722\n",
      "iteration 14600 / 30000: loss 2184.893128\n",
      "iteration 14700 / 30000: loss 2143.293755\n",
      "iteration 14800 / 30000: loss 2167.068901\n",
      "iteration 14900 / 30000: loss 2151.541085\n",
      "iteration 15000 / 30000: loss 2071.000759\n",
      "iteration 15100 / 30000: loss 2074.581279\n",
      "iteration 15200 / 30000: loss 1974.794761\n",
      "iteration 15300 / 30000: loss 1912.229878\n",
      "iteration 15400 / 30000: loss 2096.826760\n",
      "iteration 15500 / 30000: loss 1714.982084\n",
      "iteration 15600 / 30000: loss 1852.898250\n",
      "iteration 15700 / 30000: loss 2893.822613\n",
      "iteration 15800 / 30000: loss 3209.402577\n",
      "iteration 15900 / 30000: loss 1798.079504\n",
      "iteration 16000 / 30000: loss 2756.180220\n",
      "iteration 16100 / 30000: loss 1468.887361\n",
      "iteration 16200 / 30000: loss 1696.956650\n",
      "iteration 16300 / 30000: loss 1837.178816\n",
      "iteration 16400 / 30000: loss 1657.762312\n",
      "iteration 16500 / 30000: loss 1343.256139\n",
      "iteration 16600 / 30000: loss 1291.691810\n",
      "iteration 16700 / 30000: loss 2424.528299\n",
      "iteration 16800 / 30000: loss 1476.265250\n",
      "iteration 16900 / 30000: loss 1503.391604\n",
      "iteration 17000 / 30000: loss 1249.219505\n",
      "iteration 17100 / 30000: loss 2365.712151\n",
      "iteration 17200 / 30000: loss 1188.157402\n",
      "iteration 17300 / 30000: loss 7011.931510\n",
      "iteration 17400 / 30000: loss 1428.615037\n",
      "iteration 17500 / 30000: loss 2302.298698\n",
      "iteration 17600 / 30000: loss 1653.987467\n",
      "iteration 17700 / 30000: loss 1193.129124\n",
      "iteration 17800 / 30000: loss 98729.720923\n",
      "iteration 17900 / 30000: loss 22325.563259\n",
      "iteration 18000 / 30000: loss 10970.235614\n",
      "iteration 18100 / 30000: loss 6420.914136\n",
      "iteration 18200 / 30000: loss 4440.340418\n",
      "iteration 18300 / 30000: loss 3568.703529\n",
      "iteration 18400 / 30000: loss 2941.151534\n",
      "iteration 18500 / 30000: loss 2542.933903\n",
      "iteration 18600 / 30000: loss 2341.041563\n",
      "iteration 18700 / 30000: loss 2333.682045\n",
      "iteration 18800 / 30000: loss 2172.102722\n",
      "iteration 18900 / 30000: loss 2003.825890\n",
      "iteration 19000 / 30000: loss 1940.235164\n",
      "iteration 19100 / 30000: loss 1822.254545\n",
      "iteration 19200 / 30000: loss 1792.401903\n",
      "iteration 19300 / 30000: loss 1763.863760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 19400 / 30000: loss 1678.073066\n",
      "iteration 19500 / 30000: loss 1656.550321\n",
      "iteration 19600 / 30000: loss 1563.253139\n",
      "iteration 19700 / 30000: loss 1475.275255\n",
      "iteration 19800 / 30000: loss 1454.934823\n",
      "iteration 19900 / 30000: loss 1444.593037\n",
      "iteration 20000 / 30000: loss 1438.486348\n",
      "iteration 20100 / 30000: loss 1405.756997\n",
      "iteration 20200 / 30000: loss 1394.052611\n",
      "iteration 20300 / 30000: loss 1410.722723\n",
      "iteration 20400 / 30000: loss 1322.522585\n",
      "iteration 20500 / 30000: loss 1338.721881\n",
      "iteration 20600 / 30000: loss 4904.146163\n",
      "iteration 20700 / 30000: loss 1249.697703\n",
      "iteration 20800 / 30000: loss 1320.830031\n",
      "iteration 20900 / 30000: loss 1288.383713\n",
      "iteration 21000 / 30000: loss 1185.896136\n",
      "iteration 21100 / 30000: loss 1493.383105\n",
      "iteration 21200 / 30000: loss 1041.844316\n",
      "iteration 21300 / 30000: loss 1090.307823\n",
      "iteration 21400 / 30000: loss 2964.747406\n",
      "iteration 21500 / 30000: loss 1022.928082\n",
      "iteration 21600 / 30000: loss 1032.808058\n",
      "iteration 21700 / 30000: loss 1057.839135\n",
      "iteration 21800 / 30000: loss 1081.035773\n",
      "iteration 21900 / 30000: loss 1056.735243\n",
      "iteration 22000 / 30000: loss 1031.618249\n",
      "iteration 22100 / 30000: loss 4353.598103\n",
      "iteration 22200 / 30000: loss 903.201413\n",
      "iteration 22300 / 30000: loss 861.654897\n",
      "iteration 22400 / 30000: loss 857.456636\n",
      "iteration 22500 / 30000: loss 834.127919\n",
      "iteration 22600 / 30000: loss 800.711504\n",
      "iteration 22700 / 30000: loss 786.364823\n",
      "iteration 22800 / 30000: loss 764.937922\n",
      "iteration 22900 / 30000: loss 750.420168\n",
      "iteration 23000 / 30000: loss 734.920085\n",
      "iteration 23100 / 30000: loss 2542.272430\n",
      "iteration 23200 / 30000: loss 1461.402928\n",
      "iteration 23300 / 30000: loss 755.064839\n",
      "iteration 23400 / 30000: loss 705.682494\n",
      "iteration 23500 / 30000: loss 700.161284\n",
      "iteration 23600 / 30000: loss 672.643392\n",
      "iteration 23700 / 30000: loss 1482.813471\n",
      "iteration 23800 / 30000: loss 786.078600\n",
      "iteration 23900 / 30000: loss 664.799802\n",
      "iteration 24000 / 30000: loss 640.287408\n",
      "iteration 24100 / 30000: loss 604.598443\n",
      "iteration 24200 / 30000: loss 634.815692\n",
      "iteration 24300 / 30000: loss 596.680577\n",
      "iteration 24400 / 30000: loss 680.125944\n",
      "iteration 24500 / 30000: loss 1166.537055\n",
      "iteration 24600 / 30000: loss 591.611726\n",
      "iteration 24700 / 30000: loss 585.342537\n",
      "iteration 24800 / 30000: loss 575.826806\n",
      "iteration 24900 / 30000: loss 571.975136\n",
      "iteration 25000 / 30000: loss 570.132498\n",
      "iteration 25100 / 30000: loss 565.861936\n",
      "iteration 25200 / 30000: loss 1188.255192\n",
      "iteration 25300 / 30000: loss 595.944002\n",
      "iteration 25400 / 30000: loss 561.657031\n",
      "iteration 25500 / 30000: loss 557.363448\n",
      "iteration 25600 / 30000: loss 555.124247\n",
      "iteration 25700 / 30000: loss 690.920649\n",
      "iteration 25800 / 30000: loss 548.202136\n",
      "iteration 25900 / 30000: loss 1108.506503\n",
      "iteration 26000 / 30000: loss 555.056775\n",
      "iteration 26100 / 30000: loss 543.890563\n",
      "iteration 26200 / 30000: loss 539.811484\n",
      "iteration 26300 / 30000: loss 536.250418\n",
      "iteration 26400 / 30000: loss 539.784888\n",
      "iteration 26500 / 30000: loss 535.382511\n",
      "iteration 26600 / 30000: loss 531.051568\n",
      "iteration 26700 / 30000: loss 526.751868\n",
      "iteration 26800 / 30000: loss 524.045596\n",
      "iteration 26900 / 30000: loss 521.273791\n",
      "iteration 27000 / 30000: loss 1265.752409\n",
      "iteration 27100 / 30000: loss 521.531111\n",
      "iteration 27200 / 30000: loss 517.996647\n",
      "iteration 27300 / 30000: loss 517.218843\n",
      "iteration 27400 / 30000: loss 513.074405\n",
      "iteration 27500 / 30000: loss 509.799774\n",
      "iteration 27600 / 30000: loss 506.811974\n",
      "iteration 27700 / 30000: loss 511.634858\n",
      "iteration 27800 / 30000: loss 513.635639\n",
      "iteration 27900 / 30000: loss 694.269183\n",
      "iteration 28000 / 30000: loss 503.163980\n",
      "iteration 28100 / 30000: loss 497.893587\n",
      "iteration 28200 / 30000: loss 682.097363\n",
      "iteration 28300 / 30000: loss 503.164903\n",
      "iteration 28400 / 30000: loss 498.232198\n",
      "iteration 28500 / 30000: loss 500.821136\n",
      "iteration 28600 / 30000: loss 492.809014\n",
      "iteration 28700 / 30000: loss 491.356325\n",
      "iteration 28800 / 30000: loss 493.866921\n",
      "iteration 28900 / 30000: loss 492.021225\n",
      "iteration 29000 / 30000: loss 487.304862\n",
      "iteration 29100 / 30000: loss 485.883175\n",
      "iteration 29200 / 30000: loss 487.807407\n",
      "iteration 29300 / 30000: loss 485.426719\n",
      "iteration 29400 / 30000: loss 481.885202\n",
      "iteration 29500 / 30000: loss 480.658786\n",
      "iteration 29600 / 30000: loss 479.241324\n",
      "iteration 29700 / 30000: loss 477.979368\n",
      "iteration 29800 / 30000: loss 476.664373\n",
      "iteration 29900 / 30000: loss 767.062301\n",
      "Accuracy on the training set:  0.99725\n",
      "Accuracy on the testing set:  0.988\n",
      "###### top 15 words ######\n",
      "remot\n",
      "clearli\n",
      "otherwis\n",
      "mondai\n",
      "wife\n",
      "info\n",
      "with\n",
      "dollarac\n",
      "doesn\n",
      "gt\n",
      "human\n",
      "militari\n",
      "bush\n",
      "mark\n",
      "hot\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing, metrics\n",
    "import utils\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from linear_classifier import LinearSVM_twoclass\n",
    "\n",
    "# load the SPAM email training dataset\n",
    "\n",
    "N = 3600\n",
    "X,y = utils.load_mat('data/spamTrain.mat')\n",
    "yy = np.ones(y.shape)\n",
    "yy[y==0] = -1\n",
    "X_train = X[:N]\n",
    "yy_train = yy[:N]\n",
    "X_val = X[N:]\n",
    "yy_val = yy[N:]\n",
    "\n",
    "# load the SPAM email test dataset\n",
    "\n",
    "test_data = scipy.io.loadmat('data/spamTest.mat')\n",
    "X_test = test_data['Xtest']\n",
    "y_test = test_data['ytest'].flatten()\n",
    "\n",
    "##################################################################################\n",
    "#  YOUR CODE HERE for training the best performing SVM for the data above.       #\n",
    "#  what should C be? What should num_iters be? Should X be scaled?               #\n",
    "#  should X be kernelized? What should the learning rate be? What should the     #\n",
    "#  number of iterations be?                                                      #\n",
    "##################################################################################\n",
    "\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "svm = LinearSVM_twoclass()\n",
    "svm.theta = np.zeros((X.shape[1],))\n",
    "\n",
    "\n",
    "sigma_vals = [0.3,1,3,10,30,100,300]\n",
    "Cvals = [10,30,100,300,1000,3000,10000]\n",
    "learning_rates = [1e-5,3e-5,1e-4,3e-4,1e-3,3e-3,1e-2]\n",
    "\n",
    "bestAcc = 0.0\n",
    "bests = (0, 0, 0)\n",
    "\n",
    "for s in sigma_vals:\n",
    "#    KKtrain = X_train\n",
    "#    KKval = X_val\n",
    "    gamma = 1.0 / (2 * s * s)\n",
    "    Ktrain = rbf_kernel(X_train, X_train, gamma)\n",
    "    # scale the data\n",
    "    scaler = preprocessing.StandardScaler().fit(Ktrain)\n",
    "    scaleKtrain = scaler.transform(Ktrain)\n",
    "    # add the intercept term\n",
    "    KKtrain = np.vstack([np.ones((scaleKtrain.shape[0],)), scaleKtrain.T]).T\n",
    "    \n",
    "    Kval = rbf_kernel(X_val, X_train, gamma)\n",
    "    scaleKval = scaler.transform(Kval)\n",
    "    KKval = np.vstack([np.ones((scaleKval.shape[0],)), scaleKval.T]).T\n",
    "\n",
    "    svm.theta = np.zeros((KKtrain.shape[1],))\n",
    "    for c in Cvals:\n",
    "        for lr in learning_rates:\n",
    "            svm.train(KKtrain, yy_train, learning_rate=lr, reg=c, num_iters=200, batch_size=400)\n",
    "            yy_val_pre = svm.predict(KKval)\n",
    "            curAcc = np.mean((yy_val_pre == yy_val) * 1.0)\n",
    "             # print (s, c, lr, curAcc)\n",
    "            if curAcc > bestAcc:\n",
    "                bestAcc = curAcc\n",
    "                bests = (s, c, lr)\n",
    "                best_svm = svm\n",
    "            \n",
    "print (\"Best sigma, C, lr are (%f, %f, %f) with an accuracy on the validation set of %f.\" %(bests[0], bests[1], bests[2], bestAcc))\n",
    "\n",
    "##################################################################################\n",
    "# YOUR CODE HERE for testing your best model's performance                       #\n",
    "# what is the accuracy of your best model on the test set? On the training set?  #\n",
    "##################################################################################\n",
    "\n",
    "s = bests[0]\n",
    "c = bests[1]\n",
    "lr = bests[2]\n",
    "#KK = X\n",
    "gamma = 1.0 / (2 * s * s)\n",
    "K = rbf_kernel(X, X, gamma)\n",
    "scaler = preprocessing.StandardScaler().fit(K)\n",
    "scaleK = scaler.transform(K)\n",
    "KK = np.vstack([np.ones((scaleK.shape[0],)), scaleK.T]).T\n",
    "\n",
    "best_svm.theta = np.zeros((KK.shape[1],))\n",
    "best_svm.train(KK, yy, learning_rate=lr, reg=c, num_iters=30000, batch_size=KK.shape[0], verbose=True)\n",
    "yy_pre = best_svm.predict(KK)\n",
    "acc_train = np.mean((yy_pre == yy) * 1.0)\n",
    "print (\"Accuracy on the training set: \", acc_train)\n",
    "\n",
    "yy_test = np.ones(y_test.shape)\n",
    "yy_test[y_test==0] = -1\n",
    "#KKtest = X_test\n",
    "Ktest = rbf_kernel(X_test, X, gamma)\n",
    "scaleKtest = scaler.transform(Ktest)\n",
    "KKtest = np.vstack([np.ones((scaleKtest.shape[0],)), scaleKtest.T]).T\n",
    "\n",
    "yy_test_pre = best_svm.predict(KKtest)\n",
    "acc_test = np.mean((yy_test_pre == yy_test) * 1.0)\n",
    "print (\"Accuracy on the testing set: \", acc_test)\n",
    "\n",
    "##################################################################################\n",
    "# ANALYSIS OF MODEL: Print the top 15 words that are predictive of spam and for  #\n",
    "# ham. Hint: use the coefficient values of the learned model                     #\n",
    "##################################################################################\n",
    "words, inv_words = utils.get_vocab_dict()\n",
    "\n",
    "print (\"###### top 15 words ######\")\n",
    "t = np.dot(best_svm.theta[1:], X).argsort()[::-1]\n",
    "w15 = t[:15]\n",
    "for w in w15:\n",
    "    print (words[w])\n",
    "##################################################################################\n",
    "#                    END OF YOUR CODE                                            #\n",
    "##################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 0, 1, 0], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
